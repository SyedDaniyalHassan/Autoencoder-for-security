{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "one_pixel_attack",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "wNeKQemnZuZr",
        "vcwqF_XpZuZs",
        "8HIZQMslZuZt",
        "g5iT9FaCZuZv",
        "Pelj3DVDZuZv",
        "W3BC41ShZuZw",
        "7B4zH8c4ZuZx",
        "T4sIPMSZZuZ0",
        "viUTU96yZuZ0",
        "xtI-yoiWZuZ1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyedDaniyalHassan/dlp_project/blob/main/one_pixel_attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGfvzBrsZuZb"
      },
      "source": [
        "This notebook will demonstrate the one pixel attack with a few different convolutional neural network models. By using differential evolution, we find a special pixel that can modify a target image such that the network misclassifies the image (which it previously correctly classified).\n",
        "\n",
        "In theory, we want models that don't get fooled by such tiny changes. Especially in images, it is undesirable to have a small alteration in the input result in a drastic change in the output. However, even the most accurate neural networks are susceptible to this type of attack.\n",
        "\n",
        "To read more about it, see [the original paper](https://arxiv.org/abs/1710.08864), or the authors' [official repo](https://github.com/Carina02/One-Pixel-Attack).\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_7LF9vZuZc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MGoqyntZuZc"
      },
      "source": [
        "Ensure that you have `numpy`, `pandas`, `scipy`, `matplotlib`, `tensorflow-gpu`, and `keras` installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd8UZ6509otC"
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"./Dae_autoencoder_pixel_attack\")\n",
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eogg_vNLZuZd"
      },
      "source": [
        "# If running in Google Colab, import files\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    in_colab = True\n",
        "except:\n",
        "    in_colab = False\n",
        "\n",
        "if in_colab:\n",
        "    ! git clone https://github.com/SyedDaniyalHassan/dlp_project.git    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb4mwIeCactj"
      },
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "if os.getcwd() == '/content':\n",
        "  os.chdir('./Dae_autoencoder_pixel_attack')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnQbNX5KaX1K"
      },
      "source": [
        "# Python Libraries\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K\n",
        "\n",
        "# Custom Networks\n",
        "from networks.lenet import LeNet\n",
        "from networks.resnet import ResNet\n",
        "from networks.dae import Dae\n",
        "# Helper functions\n",
        "from differential_evolution import differential_evolution\n",
        "import helper\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "np.random.seed(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SGE-3XvZuZf"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2vgFkYFZuZf"
      },
      "source": [
        "For this attack, we will use the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) packaged by Keras. The task of the dataset is to correctly classify a 32x32 pixel image in 1 of 10 categories (e.g., bird, deer, truck).\n",
        "\n",
        "The code below will load the Cifar10 dataset. Keras will need to download the dataset if it is not cached locally already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjhgeOpWZuZg"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92uUh6CsZuZg"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hdoUKLXZuZg"
      },
      "source": [
        "We can access and display any image in the dataset by its index. For instance, here is a horse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMTe7p8BZuZh"
      },
      "source": [
        "image_id = 99 # Image index in the test set\n",
        "helper.plot_image(x_test[image_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG3v2TdkZuZh"
      },
      "source": [
        "## Image Perturbation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZzaMLPjZuZh"
      },
      "source": [
        "To begin, we need a function to modify one or more pixels in an image. \n",
        "\n",
        "We can define the perturbation of a pixel as a 5-tuple \n",
        "\n",
        "$$\\textbf{x} = (x, y, r, g, b)$$\n",
        "\n",
        "where $x, y$ are the coordinates of the pixel from 0 to 31, and $r,g,b$ are the red, green, and blue values from 0 to 255. Then multiple perturbations can simply be a concatenation of these tuples:\n",
        "\n",
        "$$X = (x_1, y_1, r_1, g_1, b_1, x_2, y_2, r_2, g_2, b_2, ...)$$\n",
        "\n",
        "We could instead use an array of tuples, but the optimization algorithm we will use requires it to be a flat 1-d vector.\n",
        "\n",
        "Then the function to perturb an image can take as an input the image and $X$, and output a copy of the image with each pixel at $x_i, y_i$ modified to have the color $r_i, g_i, b_i$. To speed up computation, we will batch together an array of $X$ perturbations, denoted $X_S$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEr3Y_UxZuZi"
      },
      "source": [
        "def perturb_image(xs, img):\n",
        "    # If this function is passed just one perturbation vector,\n",
        "    # pack it in a list to keep the computation the same\n",
        "    if xs.ndim < 2:\n",
        "        xs = np.array([xs])\n",
        "    \n",
        "    # Copy the image n == len(xs) times so that we can \n",
        "    # create n new perturbed images\n",
        "    tile = [len(xs)] + [1]*(xs.ndim+1)\n",
        "    imgs = np.tile(img, tile)\n",
        "    \n",
        "    # Make sure to floor the members of xs as int types\n",
        "    xs = xs.astype(int)\n",
        "    \n",
        "    for x,img in zip(xs, imgs):\n",
        "        # Split x into an array of 5-tuples (perturbation pixels)\n",
        "        # i.e., [[x,y,r,g,b], ...]\n",
        "        pixels = np.split(x, len(x) // 5)\n",
        "        for pixel in pixels:\n",
        "            # At each pixel's x,y position, assign its rgb value\n",
        "            x_pos, y_pos, *rgb = pixel\n",
        "            img[x_pos, y_pos] = rgb\n",
        "    \n",
        "    return imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYFNZFAdZuZj"
      },
      "source": [
        "Now we can modify the pixels of any image we want.\n",
        "\n",
        "Let's modify our horse image by making pixel (16,16) yellow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRhLQnTaZuZk"
      },
      "source": [
        "image_id = 99 # Image index in the test set\n",
        "pixel = np.array([16, 16, 255, 255, 0]) # pixel = x,y,r,g,b\n",
        "image_perturbed = perturb_image(pixel, x_test[image_id])[0]\n",
        "\n",
        "helper.plot_image(image_perturbed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_HG7m3YZuZk"
      },
      "source": [
        "## Load Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qAABLdUZuZm"
      },
      "source": [
        "There are two models included in this repository, `lenet` and `resnet` which will be loaded from disk automatically.\n",
        "Other models are also available but first of all train them by using train.py ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Y4aul5ZuZm"
      },
      "source": [
        "lenet = LeNet()\n",
        "resnet = ResNet()\n",
        "dae = Dae()\n",
        "models = [dae, resnet ]\n",
        "\n",
        "## Uncomment below to load more models to play with. Make sure the model files exist by training or downloading them.\n",
        "\n",
        "# lenet = LeNet()\n",
        "# pure_cnn = PureCnn()\n",
        "# net_in_net = NetworkInNetwork()\n",
        "# resnet = ResNet()\n",
        "# densenet = DenseNet()\n",
        "# wide_resnet = WideResNet()\n",
        "# capsnet = CapsNet()\n",
        "\n",
        "# models = [lenet, pure_cnn, net_in_net, resnet, densenet, wide_resnet, capsnet]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FnNPRGcZuZm"
      },
      "source": [
        "Note that there are even more networks available in the `networks` directory, but must be trained before loading them here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb9K4xj0ZuZn"
      },
      "source": [
        "### Calculate Model Accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ_6KdNeZuZn"
      },
      "source": [
        "After loading the models, we would like to evaluate all test images with each model to ensure that we only attack the images which have been classified correctly. The code below will also display the accuracy and number of parameters of each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYaLg1tuZuZn"
      },
      "source": [
        "network_stats, correct_imgs = helper.evaluate_models(models, x_test, y_test)\n",
        "correct_imgs = pd.DataFrame(correct_imgs, columns=['name', 'img', 'label', 'confidence', 'pred'])\n",
        "network_stats = pd.DataFrame(network_stats, columns=['name', 'accuracy', 'param_count'])\n",
        "\n",
        "network_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DySDcEPVZuZo"
      },
      "source": [
        "### Prediction Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzVwI0wcZuZo"
      },
      "source": [
        "For the black-box attack, all we should care about is the inputs to the model (the images), and the outputs of the model (the prediction probabilities). No special information about the model is required; we could even swap it with a model that is not a neural network.\n",
        "\n",
        "Define a function that runs several perturbed images on a given model and returns the model's confidence (probability output) in the target class, one confidence value per image. If the target class is the correct class, this will be the function that we want to minimize so that the model will be most confident in another class (which is incorrect). Otherwise, the target is an incorrect class and we will want to maximize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzC5ve6dZuZp"
      },
      "source": [
        "def predict_classes(xs, img, target_class, model, minimize=True):\n",
        "    # Perturb the image with the given pixel(s) x and get the prediction of the model\n",
        "    imgs_perturbed = perturb_image(xs, img)\n",
        "    predictions = model.predict(imgs_perturbed)[:,target_class]\n",
        "    # This function should always be minimized, so return its complement if needed\n",
        "    return predictions if minimize else 1 - predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFsJNUaCZuZq"
      },
      "source": [
        "Below we can modify a pixel in an image and see how the confidence of the model changes. In almost all cases, the confidence will not change. However, for very special cases it will change drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_AwuSaZuZq",
        "scrolled": true
      },
      "source": [
        "image_id = 384\n",
        "pixel = np.array([16, 13,  25, 48, 156])\n",
        "model = resnet\n",
        "\n",
        "true_class = y_test[image_id, 0]\n",
        "prior_confidence = model.predict_one(x_test[image_id])[true_class]\n",
        "confidence = predict_classes(pixel, x_test[image_id], true_class, model)[0]\n",
        "\n",
        "print('Confidence in true class', class_names[true_class], 'is', confidence)\n",
        "print('Prior confidence was', prior_confidence)\n",
        "helper.plot_image(perturb_image(pixel, x_test[image_id])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blyRGqqUCWuX"
      },
      "source": [
        "image_id = 384\n",
        "pixel = np.array([16, 13,  25, 48, 156])\n",
        "model = dae\n",
        "\n",
        "true_class = y_test[image_id, 0]\n",
        "prior_confidence = model.predict_one(x_test[image_id])[true_class]\n",
        "confidence = predict_classes(pixel, x_test[image_id], true_class, model)[0]\n",
        "\n",
        "print('Confidence in true class', class_names[true_class], 'is', confidence)\n",
        "print('Prior confidence was', prior_confidence)\n",
        "helper.plot_image(perturb_image(pixel, x_test[image_id])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV7u2SPoZuZq"
      },
      "source": [
        "## The Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZrSWX74ZuZr"
      },
      "source": [
        "Here we will demonstrate two variants of the one pixel attack: untargeted and targeted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNeKQemnZuZr"
      },
      "source": [
        "### Targeted vs. Untargeted Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgQIzu2xZuZr"
      },
      "source": [
        "The objective of an untargeted attack is to cause a model to misclassify an image. This means we want to perturb an image as to minimize the confidence probability of the correct classification category and maximize the sum of the probabilities of all other categories.\n",
        "\n",
        "The objective of a targeted attack is to cause a model to classify an image as a given  target class. We want to perturb an image as to maximize the probability of a class of our own choosing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcwqF_XpZuZs"
      },
      "source": [
        "### Success Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-u2_mwBZuZs"
      },
      "source": [
        "Define a function so that whenever a given perturbation is sufficient to fool a model, it returns `True`. This will be called the success criterion. The `targeted_attack` boolean flag will indicate whether success means maximization of the target class or minimization of the correct (target) class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ1FWhjPZuZs"
      },
      "source": [
        "def attack_success(x, img, target_class, model, targeted_attack=False, verbose=False):\n",
        "    # Perturb the image with the given pixel(s) and get the prediction of the model\n",
        "    attack_image = perturb_image(x, img)\n",
        "\n",
        "    confidence = model.predict(attack_image)[0]\n",
        "    predicted_class = np.argmax(confidence)\n",
        "    \n",
        "    # If the prediction is what we want (misclassification or \n",
        "    # targeted classification), return True\n",
        "    if verbose:\n",
        "        print('Confidence:', confidence[target_class])\n",
        "    if ((targeted_attack and predicted_class == target_class) or\n",
        "        (not targeted_attack and predicted_class != target_class)):\n",
        "        return True\n",
        "    # NOTE: return None otherwise (not False), due to how Scipy handles its callback function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AUwa6o_ZuZt"
      },
      "source": [
        "Here we demonstrate the usage of the success criterion function. It's nearly identical to `predict_class()` as before, but also decides the success of the attack. For purposes of demonstration we assume an untargeted attack."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVsz8XfnZuZt"
      },
      "source": [
        "image_id = 541\n",
        "pixel = np.array([17, 18, 185, 36, 215])\n",
        "model = resnet\n",
        "\n",
        "true_class = y_test[image_id, 0]\n",
        "prior_confidence = model.predict_one(x_test[image_id])[true_class]\n",
        "success = attack_success(pixel, x_test[image_id], true_class, model, verbose=True)\n",
        "\n",
        "print('Prior confidence', prior_confidence)\n",
        "print('Attack success:', success == True)\n",
        "helper.plot_image(perturb_image(pixel, x_test[image_id])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6zkXGCcKJ49"
      },
      "source": [
        "image_id = 541\n",
        "pixel = np.array([17, 18, 185, 36, 215])\n",
        "model = dae\n",
        "\n",
        "true_class = y_test[image_id, 0]\n",
        "prior_confidence = model.predict_one(x_test[image_id])[true_class]\n",
        "success = attack_success(pixel, x_test[image_id], true_class, model, verbose=True)\n",
        "\n",
        "print('Prior confidence', prior_confidence)\n",
        "print('Attack success:', success == True)\n",
        "helper.plot_image(perturb_image(pixel, x_test[image_id])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HIZQMslZuZt"
      },
      "source": [
        "### Attack Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoXZsb2cZuZt"
      },
      "source": [
        "Finally, we arrive at the attack itself: how do we find the pixels that will result in a successful attack? First, formulate it as an optimization problem: in an untargeted attack, minimize the confidence of the correct class, and in a targeted attack, maximize the confidence of a target class. This is precisely our `predict_class` function.\n",
        "\n",
        "When performing black-box optimizations such as the one pixel attack, it can be very difficult to find an efficient gradient-based optimization that will work for the problem. It would be nice to use an optimization algorithm that can find good solutions without relying on the smoothness of the function. In our case, we have discrete integer positions ranging from 0 to 31 and color intensities from 0 to 255, so the function is expected to be jagged.\n",
        "\n",
        "\n",
        "Differential evolution is a type of evolutionary algorithm where a population of candidate solutions generate offspring which compete with the rest of the population each generation according to their fitness. Each candidate solution is represented by a vector of real numbers which are the inputs to the function we would like to minimize. The lower the output of this function, the better the fitness. The algorithm works by initializing a (usually random) population of vectors, generating new offspring vectors by combining (mutating) individuals in the population, and replacing worse-performing individuals with better candidates.\n",
        "\n",
        "In the context of the one pixel attack, our input will be a flat vector of pixel values:\n",
        "\n",
        "$$X = (x_1, y_1, r_1, g_1, b_1, x_2, y_2, r_2, g_2, b_2, ...)$$\n",
        "\n",
        "These will be encoded as floating-point values, but will be floored back into integers to calculate image perturbations. First we generate a random population of $n$ perturbations\n",
        "\n",
        "$$\\textbf{P} = (X_1, X_2, \\dots, X_n)$$\n",
        "\n",
        "Then, on each iteration we calculate $n$ new mutant children using the formula\n",
        "\n",
        "$$X_i = X_{r1} + F (X_{r2} - X_{r3})$$\n",
        "\n",
        "such that\n",
        "\n",
        "$$r1 \\neq r2 \\neq r3$$\n",
        "\n",
        "where $r1,r2,r3$ are random indices into our population $\\textbf{P}$, and $F = 0.5$ is a mutation parameter. Basically, we pick 3 random individuals from the previous generation and recombine them to make a new candidate solution. If this candidate $X_i$ gives a lower minimum at position $i$ (i.e., the attack is closer to success), replace the old $X_i$ with this new one. This process repeats for several iterations until our stopping criterion, `attack_success`, which is when we find an image that successfully completes the attack.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rlzqXpqSgrW"
      },
      "source": [
        "def attack(img_id, model, target=None, pixel_count=1, \n",
        "           maxiter=75, popsize=400, verbose=False):\n",
        "    # Change the target class based on whether this is a targeted attack or not\n",
        "    targeted_attack = target is not None\n",
        "    target_class = target if targeted_attack else y_test[img_id, 0]\n",
        "    \n",
        "    # Define bounds for a flat vector of x,y,r,g,b values\n",
        "    # For more pixels, repeat this layout\n",
        "    bounds = [(0,32), (0,32), (0,256), (0,256), (0,256)] * pixel_count\n",
        "    \n",
        "    # Population multiplier, in terms of the size of the perturbation vector x\n",
        "    popmul = max(1, popsize // len(bounds))\n",
        "    \n",
        "    # Format the predict/callback functions for the differential evolution algorithm\n",
        "    def predict_fn(xs):\n",
        "        return predict_classes(xs, x_test[img_id], target_class, \n",
        "                               model, target is None)\n",
        "    \n",
        "    def callback_fn(x, convergence):\n",
        "        return attack_success(x, x_test[img_id], target_class, \n",
        "                              model, targeted_attack, verbose)\n",
        "    \n",
        "    # Call Scipy's Implementation of Differential Evolution\n",
        "    attack_result = differential_evolution(\n",
        "        predict_fn, bounds, maxiter=maxiter, popsize=popmul,\n",
        "        recombination=1, atol=-1, callback=callback_fn, polish=False)\n",
        "\n",
        "    # Calculate some useful statistics to return from this function\n",
        "    attack_image = perturb_image(attack_result.x, x_test[img_id])[0]\n",
        "    prior_probs = model.predict_one(x_test[img_id])\n",
        "    predicted_probs = model.predict_one(attack_image)\n",
        "    predicted_class = np.argmax(predicted_probs)\n",
        "    actual_class = y_test[img_id, 0]\n",
        "    success = predicted_class != actual_class\n",
        "    cdiff = prior_probs[actual_class] - predicted_probs[actual_class]\n",
        "\n",
        "    # Show the best attempt at a solution (successful or not)\n",
        "    helper.plot_image(attack_image, actual_class, class_names, predicted_class)\n",
        "\n",
        "    return [model.name, pixel_count, img_id, actual_class, predicted_class, success, cdiff, prior_probs, predicted_probs, attack_result.x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5iT9FaCZuZv"
      },
      "source": [
        "#### Untargeted Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lmGnXx3ZuZv"
      },
      "source": [
        "Let's look at one iteration of the untargeted attack. Here we will demonstrate a successful attack an image of a frog with the `resnet` model. We should see the confidence in the true class drop after several iterations.\n",
        "\n",
        "Try to see if you can successfully attack other images/models. The more pixels we are allowed to modify, the more likely it is we are to find a solution for any given image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nsg0vlHZuZv"
      },
      "source": [
        "image_id = 102\n",
        "pixels = 1 # Number of pixels to attack\n",
        "model = resnet\n",
        "\n",
        "_ = attack(image_id, model, pixel_count=pixels, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pelj3DVDZuZv"
      },
      "source": [
        "#### Targeted Attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0lvIifxZuZw"
      },
      "source": [
        "In the targeted attack, we can choose which class we want a model to classify an image as. The task is much harder for the targeted attack, as we constrain the misclassification to a given class rather than any class that's not the correct one. We should see the confidence in the target class rise after several iterations.\n",
        "\n",
        "Below we try to cause the `resnet` to classify an image of a `ship` as an `automobile`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pTCP7p4ZuZw"
      },
      "source": [
        "image_id = 108\n",
        "target_class = 1 # Integer in range 0-9\n",
        "pixels = 3\n",
        "model = resnet\n",
        "\n",
        "print('Attacking with target', class_names[target_class])\n",
        "_ = attack(image_id, model, target_class, pixel_count=pixels, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BC41ShZuZw"
      },
      "source": [
        "### Collect Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPC8n3e6ZuZw"
      },
      "source": [
        "Armed with all the necessary tools to conduct a one pixel attack, the final step is to collect relevant statistics on the targeted and untargeted attack. The relevant data points are what percentage of images were we able to successfully attack for a given model, and how the number of pixels affect this percentage.\n",
        "\n",
        "We will loop through every combination of all models, perturbations of 1,3 pixels, images, and target classes (for the targeted attack). This will take a lot of computational resources and time.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0e2m3KbZuZx"
      },
      "source": [
        "def attack_all(models, samples=500, pixels=(1,3), targeted=False, \n",
        "               maxiter=75, popsize=400, verbose=False):\n",
        "    results = []\n",
        "    for model in models:\n",
        "        model_results = []\n",
        "        valid_imgs = correct_imgs[correct_imgs.name == model.name].img\n",
        "        img_samples = np.random.choice(valid_imgs, samples, replace=False)\n",
        "        \n",
        "        for pixel_count in pixels:\n",
        "            for i, img_id in enumerate(img_samples):\n",
        "                print('\\n', model.name, '- image', img_id, '-', i+1, '/', len(img_samples))\n",
        "                targets = [None] if not targeted else range(10)\n",
        "                \n",
        "                for target in targets:\n",
        "                    if targeted:\n",
        "                        print('Attacking with target', class_names[target])\n",
        "                        if target == y_test[img_id, 0]:\n",
        "                            continue\n",
        "                    result = attack(img_id, model, target, pixel_count, \n",
        "                                    maxiter=maxiter, popsize=popsize, \n",
        "                                    verbose=verbose)\n",
        "                    model_results.append(result)\n",
        "                    \n",
        "        results += model_results\n",
        "        helper.checkpoint(results, targeted)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA_YQHeGN59Z"
      },
      "source": [
        "targeted = attack_all(models, samples=100, targeted=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssj5g-nwZuZx"
      },
      "source": [
        "targeted = attack_all(models, samples=10, targeted=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B4zH8c4ZuZx"
      },
      "source": [
        "### Attack Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI-jDiUjZuZz"
      },
      "source": [
        "Print the final results! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyb4Dw94ZuZz"
      },
      "source": [
        "# Load the results\n",
        "untargeted, targeted = helper.load_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT2yQVs8ZuZz",
        "scrolled": true
      },
      "source": [
        "columns = ['model', 'pixels', 'image', 'true', 'predicted', 'success', 'cdiff', 'prior_probs', 'predicted_probs', 'perturbation']\n",
        "untargeted_results = pd.DataFrame(untargeted, columns=columns)\n",
        "targeted_results = pd.DataFrame(targeted, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sIPMSZZuZ0"
      },
      "source": [
        "#### Untargeted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfJfz0xzZuZ0"
      },
      "source": [
        "helper.attack_stats(untargeted_results, models, network_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viUTU96yZuZ0"
      },
      "source": [
        "#### Targeted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzFPtbtQZuZ0"
      },
      "source": [
        "helper.attack_stats(targeted_results, models, network_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtI-yoiWZuZ1"
      },
      "source": [
        "### Show some successful attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqcvCFHTZuZ1"
      },
      "source": [
        "Plot 9 random successful attack images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u-Ird3lZuZ1"
      },
      "source": [
        "print('Untargeted Attack')\n",
        "helper.visualize_attack(untargeted_results, class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seOsPH9qZuZ2"
      },
      "source": [
        "print('Targeted Attack')\n",
        "helper.visualize_attack(targeted_results, class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPXDzfGTZuZ2"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDIDjJEVZuZ2"
      },
      "source": [
        "It appears that the accuracy of a model is not strongly correlated with the chance of performing a successful attack on an image. Perhaps surprisingly, the purely convolutional model is the most resistant CNN to these types of attacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuqnKnfHTNvT"
      },
      "source": [
        "# Push the Repo to the Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuJeVI0iTNvT"
      },
      "source": [
        "!git add .\n",
        "!git commit -m \"results updated!\"\n",
        "!git push -u origin main"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}